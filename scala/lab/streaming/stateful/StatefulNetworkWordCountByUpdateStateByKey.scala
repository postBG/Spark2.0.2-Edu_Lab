package lab.streaming.stateful

import org.apache.spark.SparkConf
import org.apache.spark.streaming._

/**  
 * Stateful by updateStateByKey....=> key별로 그룹핑하여 key별로 state를 유지함....=> key별로 그룹핑하여 key별로 여러 건(record)에 대해 한번에 state 연산이 처리됨....
 *   
 * 1. Socket Server(Netcat) 실행....
 *   1.1 CentOS7-14 SSH 연결(MobaXterm)
 *   1.2 # cd /kikang/spark-2.0.2-bin-hadoop2.7
 *   1.3 # yum install nc		//--Netcat(nc) 설치....(필요시....)
 *   1.4 # nc -lk 9999				//--Netcat 실행(port 9999)....   
 *   1.5 # Netcat 콘솔에 데이터 입력....	//--Netcat 데이터 송신....
 *   
 * 2. IDE에서 local 실행.... + Netcat 데이터 송신....
 *   2.1 run : Run As > Scala Application
 *   2.2 # Netcat 콘솔에 데이터 입력....	//--Netcat 데이터 송신....=> "a a a a" 형태로 입력....
 *   
 */
object StatefulNetworkWordCountByUpdateStateByKey {
  def main(args: Array[String]) {
    
    lab.common.config.Config.setHadoopHOME
    
    val sparkConf = new SparkConf().setAppName("StatefulNetworkWordCountByUpdateStateByKey").setMaster("local[*]")
    //--Create the context with a 1 second batch size
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    ssc.checkpoint("checkpoint")

    //--Create a ReceiverInputDStream on target ip:port and count the
    //--words in input stream of \n delimited test (eg. generated by 'nc')
    val lines = ssc.socketTextStream("CentOS7-14", 9999)
    val words = lines.flatMap(_.split(" "))
    val wordDstream = words.map(x => (x, 1))

    //--Update the cumulative count using updateStateByKey
    //--This will give a DStream made of state (which is the cumulative count of the words)
    val computeRunningSum = (values: Seq[Int], state: Option[Int]) => {
      val currentCount = values.foldLeft(0)(_ + _)
      val previousCount = state.getOrElse(0)
      Some(currentCount + previousCount)
    }
    
    //--key별로 그룹핑하여 key별로 여러 건(record)에 대해 한번에 state 연산이 처리됨....
    val stateDstream = wordDstream.updateStateByKey(computeRunningSum)
    
    stateDstream.print()
    
    ssc.start()
    ssc.awaitTermination()
  }
}