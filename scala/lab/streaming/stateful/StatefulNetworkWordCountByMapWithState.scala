package lab.streaming.stateful

import org.apache.spark.SparkConf
import org.apache.spark.streaming._

/**  
 * Stateful by mapWithState....=> map()처럼 동작하되 key별로 state를 유지함....=> 한 건(record)씩 state 연산이 처리됨....
 *   
 * 1. Socket Server(Netcat) 실행....
 *   1.1 CentOS7-14 SSH 연결(MobaXterm)
 *   1.2 # cd /kikang/spark-2.0.2-bin-hadoop2.7
 *   1.3 # yum install nc		//--Netcat(nc) 설치....(필요시....)
 *   1.4 # nc -lk 9999				//--Netcat 실행(port 9999)....   
 *   1.5 # Netcat 콘솔에 데이터 입력....	//--Netcat 데이터 송신....
 *   
 * 2. IDE에서 local 실행.... + Netcat 데이터 송신....
 *   2.1 run : Run As > Scala Application
 *   2.2 # Netcat 콘솔에 데이터 입력....	//--Netcat 데이터 송신....=> "a a a a" 형태로 입력....
 *   
 */
object StatefulNetworkWordCountByMapWithState {
  def main(args: Array[String]) {
    
    lab.common.config.Config.setHadoopHOME
    
    val sparkConf = new SparkConf().setAppName("StatefulNetworkWordCountByMapWithState").setMaster("local[*]")
    //--Create the context with a 1 second batch size
    val ssc = new StreamingContext(sparkConf, Seconds(1))
    ssc.checkpoint("checkpoint")

    //--Initial state RDD for mapWithState operation
    val initialRDD = ssc.sparkContext.parallelize(List(("hello", 1), ("world", 1)))

    //--Create a ReceiverInputDStream on target ip:port and count the
    //--words in input stream of \n delimited test (eg. generated by 'nc')
    val lines = ssc.socketTextStream("CentOS7-14", 9999)
    val words = lines.flatMap(_.split(" "))
    val wordDstream = words.map(x => (x, 1))

    //--Update the cumulative count using mapWithState
    //--This will give a DStream made of state (which is the cumulative count of the words)
    val mappingFunc = (word: String, one: Option[Int], state: State[Int]) => {
      val sum = one.getOrElse(0) + state.getOption.getOrElse(0)
      val output = (word, sum)
      state.update(sum)
      output
    }
    
    //--map()처럼 동작하되 key별로 state를 유지함....=> 한 건(record)씩 state 연산이 처리됨....
    val stateDstream = wordDstream.mapWithState(StateSpec.function(mappingFunc).initialState(initialRDD))
    //val stateDstream = wordDstream.mapWithState(StateSpec.function(mappingFunc).numPartitions(5))
    
    stateDstream.print()
    
    ssc.start()
    ssc.awaitTermination()
  }
}