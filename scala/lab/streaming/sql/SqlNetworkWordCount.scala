package lab.streaming.sql

import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.{Seconds, StreamingContext, Time}

/**      
 * 1. Socket Server(Netcat) 실행....
 *   1.1 CentOS7-14 SSH 연결(MobaXterm)
 *   1.2 # cd /kikang/spark-2.0.2-bin-hadoop2.7
 *   1.3 # yum install nc		//--Netcat(nc) 설치....(필요시....)
 *   1.4 # nc -lk 9999				//--Netcat 실행(port 9999)....   
 *   1.5 # Netcat 콘솔에 데이터 입력....	//--Netcat 데이터 송신....
 *   
 * 2. IDE에서 local 실행.... + Netcat 데이터 송신....
 *   2.1 run : Run As > Scala Application
 *   2.2 # Netcat 콘솔에 데이터 입력....	//--Netcat 데이터 송신....
 *   
 */
object SqlNetworkWordCount {
  def main(args: Array[String]) {
    
    //--Create the context with a 2 seconds batch size
    val sparkConf = new SparkConf().setAppName("SqlNetworkWordCount").setMaster("local[*]")
    val ssc = new StreamingContext(sparkConf, Seconds(2))

    //--Create a socket stream on target ip:port and count the
    //--words in input stream of \n delimited text (eg. generated by 'nc')
    //--Note that no duplication in storage level only for running locally.
    //--Replication necessary in distributed scenario for fault tolerance.
    val lines = ssc.socketTextStream("CentOS7-14", 9999, StorageLevel.MEMORY_AND_DISK_SER)
    val words = lines.flatMap(_.split(" "))

    //--Convert RDDs of the words DStream to DataFrame and run SQL query
    words.foreachRDD { (rdd: RDD[String], time: Time) =>
      //--Get the singleton instance of SparkSession
      val spark = SparkSessionSingleton.getInstance(rdd.sparkContext.getConf)
      import spark.implicits._

      //--Convert RDD[String] to RDD[case class] to DataFrame
      val wordsDataFrame = rdd.map(w => Record(w)).toDF()

      //--Creates a temporary view using the DataFrame
      wordsDataFrame.createOrReplaceTempView("words")

      //--Do word count on table using SQL and print it
      val wordCountsDataFrame = spark.sql("SELECT word, count(*) as total FROM words GROUP BY word")
      println(s"========= $time =========")
      wordCountsDataFrame.show()
    }

    ssc.start()
    ssc.awaitTermination()
  }
}


//--Case class for converting RDD to DataFrame....
case class Record(word: String)


//--Lazily instantiated singleton instance of SparkSession....
object SparkSessionSingleton {

  @transient  private var instance: SparkSession = _

  def getInstance(sparkConf: SparkConf): SparkSession = {
    if (instance == null) {
      instance = SparkSession.builder
                                   .config(sparkConf)
                                   .config("spark.sql.warehouse.dir", "file:///C:/Scala_IDE_for_Eclipse/eclipse/workspace/Spark2.0.2_Edu_Lab/spark-warehouse")
                                   .getOrCreate()
    }
    instance
  }
}
